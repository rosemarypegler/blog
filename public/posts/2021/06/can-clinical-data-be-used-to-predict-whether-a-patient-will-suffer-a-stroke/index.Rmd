---
title: Can Clinical Data be used to Predict Whether a Patient Will Suffer a Stroke?
author: Rose Pegler
date: '2021-06-22'
slug: can-clinical-data-be-used-to-predict-whether-a-patient-will-suffer-a-stroke
categories:
- Blog
- R
tags:
- random forest
- machine learning
- tidymodels
- R
toc: no
images: ~
---
  
```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE, collapse = TRUE)
library(tidyverse)
library(ggplot2)
library(tidymodels)
library(corrplot)
library(themis)
library(vip)
library(viridis)
library(ranger)
theme_set(theme_minimal())
```

This exercise uses the `tidymodels` package. I've watched a number of Julia Silge's Tidy Tuesday videos but haven't yet tried to conduct any machine learning using this package. This is my first time trying, and what better way to start than by looking at medical data. The data set can be found [here](https://www.kaggle.com/fedesoriano/stroke-prediction-dataset).

# Data Exploration

There's an obvious difference between those who have suffered from heart disease or hypertension and those who haven't. It makes sense that those who have had health issues are more likely to experience other issues down the line.

I can also see some pretty interesting findings. Those who have been married, are self-employed, or have smoked seem to suffer more than those who haven't. It also appears that if you work with children, or are lucky enough to not work at all, you could be less likely to suffer from a stroke.

```{r, echo = F, message=F, warning=F}
data <- read_csv("C:/Users/Rose Pegler/Documents/Blog Projects/Stroke/data/healthcare-dataset-stroke-data.csv")

data <- data %>%
  mutate(hypertension = ifelse(hypertension == 0, "No", "Yes"),
         heart_disease = ifelse(heart_disease == 0, "No", "Yes"),
         bmi = na_if(bmi, "N/A"),
         bmi = as.numeric(bmi),
         stroke = ifelse(stroke == 0, "No", "Yes")) %>%
  filter(gender != "Other")

names(data) <- c("ID", "Gender", "Age", "Hypertension", "Heart Disease", "Ever Married",
                 "Work Type", "Residence Type", "Average Glucose Level", "BMI", "Smoking Status",
                 "Stroke") 
```


```{r, echo = F, message=F, warning=F, fig.align='center', fig.height=7.5}
long_cat_data <- data %>%
  select(Stroke, Gender, Hypertension, `Heart Disease`, `Ever Married`, `Work Type`,
         `Residence Type`, `Smoking Status`) %>%
  pivot_longer(Gender:`Smoking Status`)

long_cat_data %>%
  ggplot(aes(y = value, fill = Stroke)) +
  geom_bar(position = "fill") + 
  facet_grid(rows = vars(name), scales = "free") +
  scale_x_continuous(expand = expansion(mult = c(0, .01)), labels = scales::percent) +
  labs(fill = "Did the patient \nhave a stroke?", x = "% of Variable", y = "Variable",
       title = "Proportion of people who suffered a stroke for each \ncategorical variable") +
  theme(legend.position = "bottom",
        plot.title = element_text(face = "bold"),
        strip.text.y = element_text(size = 8)) +
  scale_fill_viridis_d(direction = -1, end = 0.7)
```

Now when I inspect the numerical variables, I can see that those who have had a stroke tend to be older. This makes sense, right? I personally don't know of any young people that have had one. The glucose level of a patient could contribute, but BMI doesn't seem to be playing a huge role.

```{r, message=F, warning=F, fig.align='center', fig.width=7, fig.height=7, echo=F}
long_num_data <- data %>%
  select(Stroke, Age, `Average Glucose Level`, BMI) %>%
  pivot_longer(Age:BMI)

long_num_data %>%
  ggplot(aes(x = value, fill = Stroke)) +
  geom_boxplot(alpha = 0.7) +
  facet_wrap(~name, nrow = 3, scales = "free") +
  scale_fill_viridis_d(direction = -1, end = 0.7) +
  theme(legend.position = "bottom",
        plot.title = element_text(face = "bold")) +
  labs(fill = "Did the patient have a Stroke?", x = "Value", y = "Density",
       title = "Distribution of People Who Suffered a Stroke for Each \nNumerical Variable") 
```

# Data Preparation

In the previous plot, there were clearly a large number of outliers. I don't want to remove them for glucose level as they wouldn't be considered outliers for patients who had a stroke. However, for BMI, there are outliers for both types of patient. 

```{r}
outlier_min <- min(boxplot.stats(data$BMI)$out)

data <- data %>% 
  filter(BMI < outlier_min | is.na(BMI)) %>%
  select(-ID)
```

# Modelling

I am going to build a random forest model using the `tidymodels`, `themis` and `vip` packages.

I begin by splitting the data into a training and test set, then creating folds from the training data, stratified by the `Stroke` variable.

```{r}
set.seed(1)
split <- initial_split(data, strata = Stroke)
training <- training(split)
testing <- testing(split)

set.seed(1)
folds <- vfold_cv(training, strata = Stroke)
```

The next step is to create my recipe. This requires a number of processing steps:
  
  * Standardise the data
* The data is highly unbalanced; the number of patients who have or haven't suffered a stroke are very uneven. To deal with this I use the downsample method - remove some of the data so it is balanced.
* Impute null values using a linear regression model

I specify that I want to use a random forest model, and I want to tune the parameters to try and get the best model possible. The original kaggle task found [here](https://www.kaggle.com/fedesoriano/stroke-prediction-dataset/tasks?taskId=3281) requires a high F1 score due to the imbalanced data, so I shall pick the model based on this metric.

```{r, echo=FALSE}
recipe <- training %>%
  recipe(Stroke ~.) %>%
  step_center(all_numeric_predictors()) %>%
  step_scale(all_numeric_predictors()) %>%
  step_downsample(Stroke) %>%
  step_impute_linear(BMI)

model <- rand_forest(trees = 1000, 
                     mode = "classification",
                     mtry = tune(),
                     min_n = tune()) %>%
  set_engine("ranger")

workflow <- workflow() %>%
  add_recipe(recipe) %>%
  add_model(model)

tune_res <- readRDS( "C:/Users/Rose Pegler/Documents/Blog Projects/Stroke/data/tune_res.rds")

show_best(tune_res, metric = "f_meas")
```


```{r, warning=F, message=F, eval = F}
recipe <- training %>%
  recipe(Stroke ~.) %>%
  step_center(all_numeric_predictors()) %>%
  step_scale(all_numeric_predictors()) %>%
  step_downsample(Stroke) %>%
  step_impute_linear(BMI)

model <- rand_forest(trees = 1000, 
                     mode = "classification",
                     mtry = tune(),
                     min_n = tune()) %>%
  set_engine("ranger")

workflow <- workflow() %>%
  add_recipe(recipe) %>%
  add_model(model)

set.seed(1)
tune_res <- tune_grid(
  workflow,
  resamples = folds,
  grid = 20,
  metrics = metric_set(accuracy, sens, spec, f_meas)
)

show_best(tune_res, metric = "f_meas")
```

The model works quite well, it's over 70% accurate.

```{r}
best_tune <- select_best(tune_res, metric = 'f_meas')

final <- workflow %>%
  finalize_workflow(best_tune) %>%
  last_fit(split, metrics = metric_set(accuracy, sens, spec, f_meas))

collect_metrics(final)
```

# Important Variables

It's all very well we have a working model, but I would like to know which variables are important in the decision making process.

Age is the most important variable by far. As I saw in the EDA section, older people are more susceptible to strokes. It's more surprising that BMI actually plays a very important role within the model, when in the EDA section I couldn't see much of a relationship.

```{r, fig.align='center'}
model <- rand_forest(trees = 1000, 
                     mode = "classification",
                     mtry = best_tune$mtry,
                     min_n = best_tune$min_n) %>%
  set_engine("ranger", importance = "impurity")

workflow <- workflow() %>%
  add_recipe(recipe) %>%
  add_model(model)

final <- last_fit(workflow, split, metrics = metric_set(accuracy, roc_auc, f_meas))

final %>%
  pluck(".workflow", 1) %>%   
  pull_workflow_fit() %>% 
  vip(num_features = 20, aesthetics = list(col = "#440154FF", fill = "#440154FF"))

```



