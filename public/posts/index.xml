<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
    <channel>
        <title>Posts on Rose Pegler</title>
        <link>https://rosepegler.netlify.app/posts/</link>
        <description>Recent content in Posts on Rose Pegler</description>
        <generator>Hugo -- gohugo.io</generator>
        <language>en-us</language>
        <copyright>&lt;a href=&#34;https://creativecommons.org/licenses/by-nc/4.0/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;CC BY-NC 4.0&lt;/a&gt;</copyright>
        <lastBuildDate>Sat, 25 Sep 2021 00:00:00 +0000</lastBuildDate>
        <atom:link href="https://rosepegler.netlify.app/posts/index.xml" rel="self" type="application/rss+xml" />
        
        <item>
            <title>Visualising London&#39;s Airbnb Data</title>
            <link>https://rosepegler.netlify.app/posts/2021/09/visualising-londons-airbnb-data/</link>
            <pubDate>Sat, 25 Sep 2021 00:00:00 +0000</pubDate>
            
            <guid>https://rosepegler.netlify.app/posts/2021/09/visualising-londons-airbnb-data/</guid>
            <description>AirBnB DataOne of my favourite tasks when starting a new project is to perform exploratory data analysis. It gives me the opportunity to learn about the different variables before delving into the more ‘sciencey’ tasks.
Inside AirBnB have scraped public AirBnB data for various locations and made it available to the public. I’ve seen a number of people use these datasets for predictive analysis projects, but I want to focus on producing visualisations.</description>
            <content type="html"><![CDATA[

<div id="TOC">

</div>

<div id="airbnb-data" class="section level2">
<h2>AirBnB Data</h2>
<p>One of my favourite tasks when starting a new project is to perform exploratory data analysis. It gives me the opportunity to learn about the different variables before delving into the more ‘sciencey’ tasks.</p>
<p><a href="http://insideairbnb.com/about.html">Inside AirBnB</a> have scraped public AirBnB data for various locations and made it available to the public. I’ve seen a number of people use these datasets for predictive analysis projects, but I want to focus on producing visualisations. I also want to focus the analysis on London as it is where I’m currently living.</p>
<p>The dataset spans from 2009 to 2021 and contains information for 63,000 listings.</p>
</div>
<div id="listings-per-borough" class="section level2">
<h2>Listings per Borough</h2>
<p>The London borough with the most Airbnb listings is Westminster, and the borough with the least listings is Havering. The number of listings increase as you get nearer to the city centre, but the city centre itself doesn’t actually have that many. In Greater London, boroughs like Brent, Barnet and Haringey still have a relatively high number of listings.</p>
<p><img src="https://rosepegler.netlify.app/posts/2021/09/visualising-londons-airbnb-data/index_files/figure-html/unnamed-chunk-5-1.png" width="672" style="display: block; margin: auto;" /></p>
</div>
<div id="price-per-listing" class="section level2">
<h2>Price per Listing</h2>
<p>Now instead of looking at boroughs, I have plotted the individual listings using the longitude and latitude of the property, along with their corresponding price. I like this plot because as you get nearer to the centre the prices rise and look like city lights. You can also see the density of listings increase, and the outline of the river start to show.</p>
<p><img src="https://rosepegler.netlify.app/posts/2021/09/visualising-londons-airbnb-data/index_files/figure-html/unnamed-chunk-6-1.png" width="672" style="display: block; margin: auto;" /></p>
</div>
<div id="description-adjectives" class="section level2">
<h2>Description Adjectives</h2>
<p>I built this next plot as I was interested in seeing whether there are certain adjectives that hosts use in their property descriptions that continually have high review scores. I created this plot by taking the average review score for each adjective and ensured that the adjective had been used a ‘significant’ amount of times. For this exercise, this number was 50.</p>
<ul>
<li>The adjective “bi” has a mean score far higher than the other adjectives. This generally refers to either bi-fold doors or a bi-level property, meaning people like places with lots of light/access to the outdoors, or plenty of space.</li>
<li>Guests also appear to like places that have influences from other countries, with “Japanese”, “Scandinavian”, “German”, and “Turkish” all featuring with a high score.</li>
<li>Guests prefer leafy suburban areas where they can have some peace and quiet.</li>
</ul>
<p><img src="https://rosepegler.netlify.app/posts/2021/09/visualising-londons-airbnb-data/index_files/figure-html/unnamed-chunk-7-1.png" width="672" style="display: block; margin: auto;" /></p>
</div>
<div id="description-nouns" class="section level2">
<h2>Description Nouns</h2>
<p>I have repeated the same exercise, but this time using nouns instead of adjectives.</p>
<ul>
<li>For families, it’s important that the children have somewhere to go. Listings with a nursery or playroom score highly.</li>
<li>The appearance of the property is another key factor - people want nice artwork, architecture, and plants.</li>
<li>Guests also enjoy it when the hosts provide them with food and drink. There are multiple words in the top nouns related to this - tea, coffee, cereal, juice, salt, etc.</li>
</ul>
<p><img src="https://rosepegler.netlify.app/posts/2021/09/visualising-londons-airbnb-data/index_files/figure-html/unnamed-chunk-8-1.png" width="672" style="display: block; margin: auto;" /></p>
</div>
<div id="amenities" class="section level2">
<h2>Amenities</h2>
<p>The same exercise has been repeated but instead I will look at the amenities that listings have included. There are over 1,000 unique amenities included within this dataset.</p>
<p>With ‘Bathroom essentials’ and ‘Bedroom comforts’ are slightly ambiguous, many of the top scoring amenities centre around entertainment with people finding enjoyment through board games, sound systems, and a fire pit. The rest of the top-scoring amenities are either practical, such as cleaning products, or nice-to-haves, such as a Nespresso coffee machine.</p>
<p><img src="https://rosepegler.netlify.app/posts/2021/09/visualising-londons-airbnb-data/index_files/figure-html/unnamed-chunk-9-1.png" width="672" style="display: block; margin: auto;" /></p>
</div>
<div id="house-prices-vs-airbnb" class="section level2">
<h2>House Prices vs Airbnb</h2>
<p>This next plot compares the average house price for each borough with the average Airbnb price in that borough. House price data has been taken from <a href="https://data.london.gov.uk/dataset/average-house-prices">here</a>. I had to take 2019 data as it is the latest available.</p>
<p>The observations follow an expected trajectory, but there are a couple of points to note.</p>
<p>Kensington &amp; Chelsea and Westminster are way out on their own in terms of both house prices and Airbnb prices. They are clearly desirable places to live but I probably wouldn’t bother coughing up the cash to stay here when there are so many other places in London.</p>
<p>Camden is a more desirable place to live than in the city centre of London (desirability being based off house prices) but it is cheaper to stay in. Similar places include Hammersmith &amp; Fulham and Richmond upon Thames.
Southwark, Lambeth, Tower Hamlets and Hackney are right near the city centre but it’s far cheaper to in an Airbnb there compared to the others.</p>
<p>Haringey’s house prices are similar to some of the boroughs near the city centre, but it is one of the cheapest boroughs to stay in.</p>
<p><img src="https://rosepegler.netlify.app/posts/2021/09/visualising-londons-airbnb-data/index_files/figure-html/unnamed-chunk-10-1.png" width="672" style="display: block; margin: auto;" /></p>
</div>
<div id="adjectives-used-in-boroughs" class="section level2">
<h2>Adjectives used in Boroughs</h2>
<p>Inside Airbnb also offer a reviews dataset that tells us the comments left in reviews for each listing. I wanted to see whether adjectives in reviews differ across boroughs. I chose 5 boroughs for the exercise - Barking and Dagenham, Kingston upon Thames, Wandsworth, Croydon, and Hackney.</p>
<p>I originally tried looking at this by finding the most frequently occurring adjectives for each borough, but these didn’t really differ across them - in reviews, people regularly say the listing is nice or lovely, but it was difficult to find any differing language. Instead, I have used the <code>tidylo</code> package which calculates the weighted log odds for each terms in each borough. This tells us the words which are more likely to appear in reviews in each borough.</p>
<ul>
<li>Hackney stands out from the others as a <em>trendy</em> place to stay, somewhere that people should go if they want to be in a <em>vibrant</em> area.</li>
<li>Kingston upon Thames offers up <em>gorgeous</em> homes. Interestingly, <em>Korean</em> comes up - Kingston upon Thames has a large South Korean community, meaning there’s lots of Korean hosts and places to eat.</li>
<li><em>Holy</em> appearing in Wandsworth adjectives is actually people referring the Holy Cow Indian restaurant rather than anything religious.</li>
<li>Barking and Dagenham reviews contain the usual adjectives. Perhaps you’re more likely to get a clean and comfortable apartment.</li>
<li>Croydon places are <em>snug</em> and the hosts tend to be <em>reliable</em>.</li>
</ul>
<p><img src="https://rosepegler.netlify.app/posts/2021/09/visualising-londons-airbnb-data/index_files/figure-html/unnamed-chunk-12-1.png" width="100%" height="100%" style="display: block; margin: auto;" /></p>
</div>
<div id="reviews-over-time" class="section level2">
<h2>Reviews over time</h2>
<p>In this final plot, I wanted to see how the frequency of reviews over time changed. The number of Airbnbs being booked increased year-on-year until March 2020, when the UK went into lockdown. Between April and June 2020 there were virtually no Airbnbs being booked. The number increased but decreased again during Novemeber and December when we went other lockdowns. By April 2021 the number being booked shot up again. It’s also interesting to see how the number of places being booked peaks in July every year and, for some reason, October also seems to be a pretty popular month, perhaps because of half-terms.</p>
<p><img src="https://rosepegler.netlify.app/posts/2021/09/visualising-londons-airbnb-data/index_files/figure-html/unnamed-chunk-13-1.png" width="672" style="display: block; margin: auto;" /></p>
<p>The code for this post can be found <a href="https://github.com/rosemarypegler/blog/tree/main/content">here</a>.</p>
</div>
]]></content>
        </item>
        
        <item>
            <title>Can Clinical Data be used to Predict Whether a Patient Will Suffer a Stroke?</title>
            <link>https://rosepegler.netlify.app/posts/2021/06/can-clinical-data-be-used-to-predict-whether-a-patient-will-suffer-a-stroke/</link>
            <pubDate>Tue, 22 Jun 2021 00:00:00 +0000</pubDate>
            
            <guid>https://rosepegler.netlify.app/posts/2021/06/can-clinical-data-be-used-to-predict-whether-a-patient-will-suffer-a-stroke/</guid>
            <description>This exercise uses the tidymodels package. I’ve watched a number of Julia Silge’s Tidy Tuesday videos but haven’t yet tried to conduct any machine learning using this package. This is my first time trying, and what better way to start than by looking at medical data. The data set can be found here.
Data ExplorationThere’s an obvious difference between those who have suffered from heart disease or hypertension and those who haven’t.</description>
            <content type="html"><![CDATA[

<div id="TOC">

</div>

<p>This exercise uses the <code>tidymodels</code> package. I’ve watched a number of Julia Silge’s Tidy Tuesday videos but haven’t yet tried to conduct any machine learning using this package. This is my first time trying, and what better way to start than by looking at medical data. The data set can be found <a href="https://www.kaggle.com/fedesoriano/stroke-prediction-dataset">here</a>.</p>
<div id="data-exploration" class="section level1">
<h1>Data Exploration</h1>
<p>There’s an obvious difference between those who have suffered from heart disease or hypertension and those who haven’t. It makes sense that those who have had health issues are more likely to experience other issues down the line.</p>
<p>I can also see some pretty interesting findings. Those who have been married, are self-employed, or have smoked seem to suffer more than those who haven’t. It also appears that if you work with children, or are lucky enough to not work at all, you could be less likely to suffer from a stroke.</p>
<p><img src="https://rosepegler.netlify.app/posts/2021/06/can-clinical-data-be-used-to-predict-whether-a-patient-will-suffer-a-stroke/index_files/figure-html/unnamed-chunk-2-1.png" width="672" style="display: block; margin: auto;" /></p>
<p>Now when I inspect the numerical variables, I can see that those who have had a stroke tend to be older. This makes sense, right? I personally don’t know of any young people that have had one. The glucose level of a patient could contribute, but BMI doesn’t seem to be playing a huge role.</p>
<p><img src="https://rosepegler.netlify.app/posts/2021/06/can-clinical-data-be-used-to-predict-whether-a-patient-will-suffer-a-stroke/index_files/figure-html/unnamed-chunk-3-1.png" width="672" style="display: block; margin: auto;" /></p>
</div>
<div id="data-preparation" class="section level1">
<h1>Data Preparation</h1>
<p>In the previous plot, there were clearly a large number of outliers. I don’t want to remove them for glucose level as they wouldn’t be considered outliers for patients who had a stroke. However, for BMI, there are outliers for both types of patient.</p>
<pre class="r"><code>outlier_min &lt;- min(boxplot.stats(data$BMI)$out)

data &lt;- data %&gt;% 
  filter(BMI &lt; outlier_min | is.na(BMI)) %&gt;%
  select(-ID)</code></pre>
</div>
<div id="modelling" class="section level1">
<h1>Modelling</h1>
<p>I am going to build a random forest model using the <code>tidymodels</code>, <code>themis</code> and <code>vip</code> packages.</p>
<p>I begin by splitting the data into a training and test set, then creating folds from the training data, stratified by the <code>Stroke</code> variable.</p>
<pre class="r"><code>set.seed(1)
split &lt;- initial_split(data, strata = Stroke)
training &lt;- training(split)
testing &lt;- testing(split)

set.seed(1)
folds &lt;- vfold_cv(training, strata = Stroke)</code></pre>
<p>The next step is to create my recipe. This requires a number of processing steps:</p>
<ul>
<li>Standardise the data</li>
<li>The data is highly unbalanced; the number of patients who have or haven’t suffered a stroke are very uneven. To deal with this I use the downsample method - remove some of the data so it is balanced.</li>
<li>Impute null values using a linear regression model</li>
</ul>
<p>I specify that I want to use a random forest model, and I want to tune the parameters to try and get the best model possible. The original kaggle task found <a href="https://www.kaggle.com/fedesoriano/stroke-prediction-dataset/tasks?taskId=3281">here</a> requires a high F1 score due to the imbalanced data, so I shall pick the model based on this metric.</p>
<pre><code>## # A tibble: 5 x 8
##    mtry min_n .metric .estimator  mean     n std_err .config              
##   &lt;int&gt; &lt;int&gt; &lt;chr&gt;   &lt;chr&gt;      &lt;dbl&gt; &lt;int&gt;   &lt;dbl&gt; &lt;chr&gt;                
## 1     5    14 f_meas  binary     0.826    10 0.00666 Preprocessor1_Model01
## 2     3    10 f_meas  binary     0.826    10 0.00624 Preprocessor1_Model19
## 3     5     8 f_meas  binary     0.826    10 0.00603 Preprocessor1_Model03
## 4     9    24 f_meas  binary     0.825    10 0.00581 Preprocessor1_Model07
## 5     6     4 f_meas  binary     0.825    10 0.00617 Preprocessor1_Model05</code></pre>
<pre class="r"><code>recipe &lt;- training %&gt;%
  recipe(Stroke ~.) %&gt;%
  step_center(all_numeric_predictors()) %&gt;%
  step_scale(all_numeric_predictors()) %&gt;%
  step_downsample(Stroke) %&gt;%
  step_impute_linear(BMI)

model &lt;- rand_forest(trees = 1000, 
                     mode = &quot;classification&quot;,
                     mtry = tune(),
                     min_n = tune()) %&gt;%
  set_engine(&quot;ranger&quot;)

workflow &lt;- workflow() %&gt;%
  add_recipe(recipe) %&gt;%
  add_model(model)

set.seed(1)
tune_res &lt;- tune_grid(
  workflow,
  resamples = folds,
  grid = 20,
  metrics = metric_set(accuracy, sens, spec, f_meas)
)

show_best(tune_res, metric = &quot;f_meas&quot;)</code></pre>
<p>The model works quite well, it’s over 70% accurate.</p>
<pre class="r"><code>best_tune &lt;- select_best(tune_res, metric = &#39;f_meas&#39;)

final &lt;- workflow %&gt;%
  finalize_workflow(best_tune) %&gt;%
  last_fit(split, metrics = metric_set(accuracy, sens, spec, f_meas))
## Warning: package &#39;vctrs&#39; was built under R version 4.0.5

collect_metrics(final)
## # A tibble: 4 x 4
##   .metric  .estimator .estimate .config             
##   &lt;chr&gt;    &lt;chr&gt;          &lt;dbl&gt; &lt;chr&gt;               
## 1 accuracy binary         0.715 Preprocessor1_Model1
## 2 sens     binary         0.713 Preprocessor1_Model1
## 3 spec     binary         0.764 Preprocessor1_Model1
## 4 f_meas   binary         0.827 Preprocessor1_Model1</code></pre>
</div>
<div id="important-variables" class="section level1">
<h1>Important Variables</h1>
<p>It’s all very well we have a working model, but I would like to know which variables are important in the decision making process.</p>
<p>Age is the most important variable by far. As I saw in the EDA section, older people are more susceptible to strokes. It’s more surprising that BMI actually plays a very important role within the model, when in the EDA section I couldn’t see much of a relationship.</p>
<pre class="r"><code>model &lt;- rand_forest(trees = 1000, 
                     mode = &quot;classification&quot;,
                     mtry = best_tune$mtry,
                     min_n = best_tune$min_n) %&gt;%
  set_engine(&quot;ranger&quot;, importance = &quot;impurity&quot;)

workflow &lt;- workflow() %&gt;%
  add_recipe(recipe) %&gt;%
  add_model(model)

final &lt;- last_fit(workflow, split, metrics = metric_set(accuracy, roc_auc, f_meas))

final %&gt;%
  pluck(&quot;.workflow&quot;, 1) %&gt;%   
  pull_workflow_fit() %&gt;% 
  vip(num_features = 20, aesthetics = list(col = &quot;#440154FF&quot;, fill = &quot;#440154FF&quot;))</code></pre>
<p><img src="https://rosepegler.netlify.app/posts/2021/06/can-clinical-data-be-used-to-predict-whether-a-patient-will-suffer-a-stroke/index_files/figure-html/unnamed-chunk-9-1.png" width="672" style="display: block; margin: auto;" /></p>
</div>
]]></content>
        </item>
        
    </channel>
</rss>
